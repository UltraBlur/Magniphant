#!/usr/bin/env python3
"""
Eulerian Video Magnification Core Module - Windows Compatible Version
æ¬§æ‹‰è§†é¢‘æ”¾å¤§æ ¸å¿ƒæ¨¡å— - Windowså…¼å®¹ç‰ˆæœ¬
"""

import numpy as np
import cv2
import subprocess
import os
from scipy import signal
from collections import deque
import concurrent.futures
import threading
import warnings
warnings.filterwarnings('ignore')

# é›†æˆeulerian-magnificationåº“ç”¨äºé¢‘ç‡åˆ†æ
try:
    import eulerian_magnification as em
    HAS_EM_LIB = True
    print("eulerian-magnificationåº“å·²åŠ è½½")
except ImportError:
    HAS_EM_LIB = False
    print("eulerian-magnificationåº“æœªå®‰è£…ï¼Œé¢‘ç‡åˆ†æåŠŸèƒ½ä¸å¯ç”¨")

# å¯ç”¨numpyä¼˜åŒ–
np.seterr(all='ignore')


class EulerianVideoMagnification:
    """æ¬§æ‹‰è§†é¢‘æ”¾å¤§æ ¸å¿ƒç±» - Windowså…¼å®¹é«˜æ€§èƒ½ç‰ˆæœ¬"""

    def __init__(self, video_path, output_path="output.mp4", buffer_size=150, num_workers=None):
        self.video_path = video_path
        self.output_path = output_path
        self.fps = None
        self.width = None
        self.height = None
        self.total_frames = None
        # å¤§å¹…å‡å°‘ç¼“å†²åŒºé¿å…å†…å­˜æº¢å‡º - 64GBéƒ½ä¸å¤Ÿè¯´æ˜æœ‰ä¸¥é‡é—®é¢˜
        self.buffer_size = min(30, buffer_size)  # æœ€å¤š30å¸§ï¼Œé˜²æ­¢å†…å­˜çˆ†ç‚¸
        # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒï¼Œä½†åœ¨Windowsä¸Šä½¿ç”¨ThreadPoolExecutoræ›´ç¨³å®š
        import multiprocessing as mp
        self.num_workers = num_workers or mp.cpu_count()
        # åˆ›å»ºæŒä¹…çº¿ç¨‹æ± é¿å…é‡å¤åˆ›å»ºå¼€é”€
        self.executor = None
        print(f"åˆå§‹åŒ–å¤„ç†å™¨ï¼Œä½¿ç”¨ {self.num_workers} ä¸ªå·¥ä½œçº¿ç¨‹")

    def __del__(self):
        """ææ„å‡½æ•° - ç¡®ä¿çº¿ç¨‹æ± è¢«æ¸…ç†"""
        self._cleanup_executor()

    def _init_executor(self):
        """åˆå§‹åŒ–æŒä¹…çº¿ç¨‹æ± """
        if self.executor is None:
            self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=self.num_workers)

    def _cleanup_executor(self):
        """æ¸…ç†çº¿ç¨‹æ± """
        if self.executor is not None:
            self.executor.shutdown(wait=True)
            self.executor = None

    def get_video_info(self):
        """è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯å¹¶æ£€æµ‹è¶…é«˜åˆ†è¾¨ç‡"""
        cap = cv2.VideoCapture(self.video_path)
        self.fps = int(cap.get(cv2.CAP_PROP_FPS))
        self.width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        self.total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        cap.release()

        # è®¡ç®—åˆ†è¾¨ç‡ç­‰çº§å’Œå†…å­˜éœ€æ±‚
        total_pixels = self.width * self.height
        frame_size_mb = (total_pixels * 3 * 4) / (1024 * 1024)  # float32 RGB

        print(f"è§†é¢‘: {self.width}x{self.height}, {self.fps}FPS, {self.total_frames}å¸§")
        print(f"æ¯å¸§å†…å­˜: {frame_size_mb:.1f}MB")

        # è¶…é«˜åˆ†è¾¨ç‡æ£€æµ‹å’Œè­¦å‘Š
        if total_pixels > 33177600:  # 8K (7680x4320)
            print(f"âš ï¸ æ£€æµ‹åˆ°è¶…é«˜åˆ†è¾¨ç‡è§†é¢‘ ({self.width}x{self.height})")
            print("è‡ªåŠ¨å¯ç”¨è¶…é«˜åˆ†è¾¨ç‡ä¼˜åŒ–æ¨¡å¼...")

            # å¼ºåˆ¶å‡å°‘ç¼“å†²åŒºåˆ°æœ€å°
            self.buffer_size = min(10, self.buffer_size)
            self.is_ultra_high_res = True

            if total_pixels > 100663296:  # 12K+
                print("ğŸš¨ 12K+åˆ†è¾¨ç‡æ£€æµ‹ï¼å¼ºåˆ¶å¯ç”¨æé™å†…å­˜æ¨¡å¼")
                self.buffer_size = 5  # æœ€å°ç¼“å†²åŒº
                self.extreme_mode = True
        else:
            self.is_ultra_high_res = False
            self.extreme_mode = False

        return self.fps, self.width, self.height

    def analyze_video_frequencies(self, max_frames=300):
        """ä½¿ç”¨eulerian-magnificationåº“åˆ†æè§†é¢‘é¢‘ç‡"""
        if not HAS_EM_LIB:
            print("eulerian-magnificationåº“æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œé¢‘ç‡åˆ†æ")
            return None

        try:
            print(f"\nå¼€å§‹é¢‘ç‡åˆ†æ...")
            print(f"åˆ†æå¸§æ•°: {min(max_frames, self.total_frames)} å¸§")

            # ä½¿ç”¨OpenCVç›´æ¥åŠ è½½è§†é¢‘è¿›è¡Œé¢‘ç‡åˆ†æ
            cap = cv2.VideoCapture(self.video_path)
            frames = []
            frame_count = 0

            while frame_count < max_frames:
                ret, frame = cap.read()
                if not ret:
                    break

                # è½¬æ¢ä¸ºæµ®ç‚¹æ•°å¹¶å½’ä¸€åŒ–
                frame_float = frame.astype(np.float32) / 255.0
                frames.append(frame_float)
                frame_count += 1

            cap.release()

            if len(frames) < 10:
                print("è§†é¢‘å¸§æ•°å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œé¢‘ç‡åˆ†æ")
                return None

            vid = np.array(frames)
            print(f"è§†é¢‘æ•°æ®: {vid.shape}, FPS: {self.fps}")

            # ä½¿ç”¨è‡ªå®šä¹‰é¢‘ç‡åˆ†æå‡½æ•°ï¼ˆä¿®å¤åº“çš„Python 3å…¼å®¹æ€§é—®é¢˜ï¼‰
            print("æ­£åœ¨åˆ†æé¢‘ç‡æˆåˆ†...")
            frequency_data = self._analyze_frequencies_custom(vid, self.fps)

            # è§£æé¢‘ç‡åˆ†æç»“æœ
            if frequency_data is not None:
                # æ‰¾åˆ°ä¸»è¦é¢‘ç‡æˆåˆ†
                dominant_frequencies = self._extract_dominant_frequencies(frequency_data, self.fps)

                print(f"\né¢‘ç‡åˆ†æç»“æœ:")
                for freq_info in dominant_frequencies:
                    print(f"  ä¸»è¦é¢‘ç‡: {freq_info['frequency']:.2f} Hz (å¼ºåº¦: {freq_info['magnitude']:.3f})")

                return {
                    'fps': self.fps,
                    'dominant_frequencies': dominant_frequencies,
                    'raw_data': frequency_data,
                    'analyzed_frames': len(vid)
                }
            else:
                print("é¢‘ç‡åˆ†æå¤±è´¥")
                return None

        except Exception as e:
            print(f"é¢‘ç‡åˆ†æå‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _analyze_frequencies_custom(self, vid, fps):
        """è‡ªå®šä¹‰é¢‘ç‡åˆ†æå‡½æ•°ï¼ˆä¿®å¤Python 3å…¼å®¹æ€§é—®é¢˜ï¼‰"""
        try:
            # å°†è§†é¢‘è½¬æ¢ä¸ºç°åº¦å¹¶è®¡ç®—æ¯ä¸ªåƒç´ çš„æ—¶é—´åºåˆ—
            if len(vid.shape) == 4:  # (frames, height, width, channels)
                # è½¬æ¢ä¸ºç°åº¦
                vid_gray = np.mean(vid, axis=3)
            else:
                vid_gray = vid

            # è®¡ç®—å…¨å±€å¹³å‡äº®åº¦çš„æ—¶é—´åºåˆ—
            temporal_signal = np.mean(vid_gray, axis=(1, 2))

            # åº”ç”¨FFTåˆ†æé¢‘ç‡
            n_frames = len(temporal_signal)
            fft_result = np.fft.fft(temporal_signal)

            # è®¡ç®—é¢‘ç‡è½´ï¼ˆä¿®å¤æ•´æ•°é™¤æ³•é—®é¢˜ï¼‰
            freqs = np.fft.fftfreq(n_frames, 1.0/fps)

            # åªå–æ­£é¢‘ç‡éƒ¨åˆ†ï¼ˆä¿®å¤slice indicesé—®é¢˜ï¼‰
            positive_idx = n_frames // 2 + 1  # ä½¿ç”¨æ•´æ•°é™¤æ³•
            freqs = freqs[:positive_idx]
            fft_magnitude = np.abs(fft_result[:positive_idx])

            return {
                'frequencies': freqs,
                'magnitudes': fft_magnitude,
                'temporal_signal': temporal_signal
            }

        except Exception as e:
            print(f"è‡ªå®šä¹‰é¢‘ç‡åˆ†æå‡ºé”™: {e}")
            return None

    def _extract_dominant_frequencies(self, frequency_data, fps, top_n=5):
        """ä»é¢‘ç‡æ•°æ®ä¸­æå–ä¸»è¦é¢‘ç‡æˆåˆ†"""
        try:
            # å¤„ç†è‡ªå®šä¹‰é¢‘ç‡åˆ†æçš„è¿”å›æ ¼å¼
            if isinstance(frequency_data, dict) and 'frequencies' in frequency_data:
                freqs = frequency_data['frequencies']
                magnitudes = frequency_data['magnitudes']

                # è¿‡æ»¤æ‰DCåˆ†é‡å’Œè¿‡ä½é¢‘ç‡
                valid_indices = np.where((freqs > 0.1) & (freqs < fps/2))[0]

                if len(valid_indices) == 0:
                    print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„é¢‘ç‡æˆåˆ†")
                    return []

                valid_freqs = freqs[valid_indices]
                valid_magnitudes = magnitudes[valid_indices]

                # æ‰¾åˆ°æœ€å¼ºçš„é¢‘ç‡æˆåˆ†
                peak_indices = np.argsort(valid_magnitudes)[-top_n:][::-1]

                dominant_frequencies = []
                for idx in peak_indices:
                    if idx < len(valid_freqs):
                        dominant_frequencies.append({
                            'frequency': valid_freqs[idx],
                            'magnitude': valid_magnitudes[idx],
                            'index': valid_indices[idx]
                        })

                return dominant_frequencies
            else:
                print("æœªçŸ¥çš„é¢‘ç‡æ•°æ®æ ¼å¼")
                return []

        except Exception as e:
            print(f"æå–ä¸»è¦é¢‘ç‡æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return []

    def suggest_frequency_range(self, analysis_result=None):
        """åŸºäºé¢‘ç‡åˆ†æç»“æœå»ºè®®æœ€ä½³é¢‘ç‡èŒƒå›´"""
        if analysis_result is None:
            analysis_result = self.analyze_video_frequencies()

        if analysis_result is None or not analysis_result['dominant_frequencies']:
            print("âš ï¸ æ— é¢‘ç‡åˆ†ææ•°æ®ï¼Œä½¿ç”¨é»˜è®¤èŒƒå›´")
            return {'freq_low': 0.4, 'freq_high': 3.0, 'confidence': 'low'}

        dominant_freqs = analysis_result['dominant_frequencies']

        # æ‰¾åˆ°æœ€å¼ºçš„é¢‘ç‡æˆåˆ†
        strongest_freq = dominant_freqs[0]['frequency']

        # åŸºäºæœ€å¼ºé¢‘ç‡å»ºè®®èŒƒå›´
        if strongest_freq < 1.0:
            # ä½é¢‘è¿åŠ¨ (å¦‚å‘¼å¸)
            suggested_low = max(0.1, strongest_freq - 0.3)
            suggested_high = min(2.0, strongest_freq + 0.5)
            motion_type = "å‘¼å¸æˆ–æ…¢é€Ÿè¿åŠ¨"
        elif strongest_freq < 3.0:
            # ä¸­é¢‘è¿åŠ¨ (å¦‚å¿ƒè·³)
            suggested_low = max(0.5, strongest_freq - 0.5)
            suggested_high = min(4.0, strongest_freq + 1.0)
            motion_type = "å¿ƒè·³æˆ–ä¸­é€Ÿè¿åŠ¨"
        else:
            # é«˜é¢‘è¿åŠ¨
            suggested_low = max(1.0, strongest_freq - 1.0)
            suggested_high = min(8.0, strongest_freq + 2.0)
            motion_type = "å¿«é€Ÿè¿åŠ¨æˆ–æŒ¯åŠ¨"

        print(f"\nå»ºè®®çš„é¢‘ç‡èŒƒå›´:")
        print(f"  æ£€æµ‹åˆ°ä¸»è¦é¢‘ç‡: {strongest_freq:.2f} Hz ({motion_type})")
        print(f"  å»ºè®®èŒƒå›´: {suggested_low:.1f} - {suggested_high:.1f} Hz")

        return {
            'freq_low': suggested_low,
            'freq_high': suggested_high,
            'dominant_frequency': strongest_freq,
            'motion_type': motion_type,
            'confidence': 'high'
        }

    def build_gaussian_pyramid(self, frame, levels=4):
        """æ„å»ºé«˜æ–¯é‡‘å­—å¡” - cv2ä¼˜åŒ–ç‰ˆæœ¬"""
        pyramid = [frame.astype(np.float32)]
        current = frame.astype(np.float32)

        # ä½¿ç”¨cv2.pyrDownï¼Œæ¯”scipyæ›´å¿«ä¸”æ›´ç¨³å®š
        for i in range(levels - 1):
            current = cv2.pyrDown(current)
            pyramid.append(current)
        return pyramid

    def build_laplacian_pyramid(self, frame, levels=4):
        """æ„å»ºæ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡” - æ­£ç¡®çš„è¿åŠ¨æ”¾å¤§æ–¹æ³•"""
        # å…ˆæ„å»ºé«˜æ–¯é‡‘å­—å¡”
        gaussian_pyramid = self.build_gaussian_pyramid(frame, levels)

        # æ„å»ºæ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”
        laplacian_pyramid = []

        for i in range(levels - 1):
            # ä¸Šé‡‡æ ·ä¸‹ä¸€å±‚
            size = (gaussian_pyramid[i].shape[1], gaussian_pyramid[i].shape[0])
            upsampled = cv2.pyrUp(gaussian_pyramid[i + 1], dstsize=size)

            # æ‹‰æ™®æ‹‰æ–¯å±‚ = å½“å‰é«˜æ–¯å±‚ - ä¸Šé‡‡æ ·çš„ä¸‹ä¸€å±‚
            laplacian = gaussian_pyramid[i] - upsampled
            laplacian_pyramid.append(laplacian)

        # æœ€åä¸€å±‚å°±æ˜¯é«˜æ–¯é‡‘å­—å¡”çš„æœ€åä¸€å±‚ï¼ˆæœ€ç²—ç³™çš„å±‚ï¼‰
        laplacian_pyramid.append(gaussian_pyramid[-1])

        return laplacian_pyramid

    def apply_temporal_bandpass_filter_fft(self, data, fps, freq_low, freq_high, amplification=1):
        """ä½¿ç”¨FFTè¿›è¡Œæ—¶åŸŸå¸¦é€šæ»¤æ³¢ - å‚è€ƒeulerian_magnificationåº“çš„æ­£ç¡®å®ç°"""
        try:
            # data shape: (frames, height, width, channels)
            print(f"åº”ç”¨FFTå¸¦é€šæ»¤æ³¢: {freq_low}-{freq_high} Hz, æ”¾å¤§å€æ•°: {amplification}x")

            # ä½¿ç”¨å®æ•°FFTï¼ˆæ›´é«˜æ•ˆï¼‰
            fft = np.fft.rfft(data, axis=0)
            frequencies = np.fft.fftfreq(data.shape[0], d=1.0 / fps)

            # æ‰¾åˆ°é¢‘ç‡è¾¹ç•Œçš„ç´¢å¼•
            bound_low = (np.abs(frequencies - freq_low)).argmin()
            bound_high = (np.abs(frequencies - freq_high)).argmin()

            # åˆ›å»ºå¸¦é€šæ»¤æ³¢å™¨ï¼šåªä¿ç•™æŒ‡å®šé¢‘ç‡èŒƒå›´
            fft[:bound_low] = 0
            fft[bound_high:-bound_high] = 0
            fft[-bound_low:] = 0

            # é€†FFTæ¢å¤æ—¶åŸŸä¿¡å·
            result = np.fft.irfft(fft, n=data.shape[0], axis=0)

            # åº”ç”¨æ”¾å¤§ç³»æ•°
            result = result.real * amplification

            return result.astype(np.float32)

        except Exception as e:
            print(f"FFTæ»¤æ³¢å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return np.zeros_like(data)

    def create_laplacian_video_pyramid(self, video_frames, levels=4):
        """åˆ›å»ºæ•´ä¸ªè§†é¢‘çš„æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡” - å‚è€ƒåº“çš„æ­£ç¡®å®ç°"""
        print(f"æ„å»ºæ‹‰æ™®æ‹‰æ–¯è§†é¢‘é‡‘å­—å¡”ï¼Œå±‚æ•°: {levels}")

        vid_pyramid = []
        frame_count = len(video_frames)

        # å¯¹æ¯ä¸€å¸§æ„å»ºé‡‘å­—å¡”
        for frame_idx, frame in enumerate(video_frames):
            frame_pyramid = self.build_laplacian_pyramid(frame, levels)

            # åˆå§‹åŒ–é‡‘å­—å¡”ç»“æ„
            if frame_idx == 0:
                for level in range(levels):
                    h, w = frame_pyramid[level].shape[:2]
                    vid_pyramid.append(np.zeros((frame_count, h, w, 3), dtype=np.float32))

            # å°†å½“å‰å¸§çš„æ¯å±‚æ·»åŠ åˆ°å¯¹åº”çš„è§†é¢‘é‡‘å­—å¡”å±‚
            for level in range(levels):
                vid_pyramid[level][frame_idx] = frame_pyramid[level]

            if (frame_idx + 1) % 50 == 0:
                print(f"  å·²å¤„ç† {frame_idx + 1}/{frame_count} å¸§")

        return vid_pyramid

    def collapse_laplacian_pyramid(self, image_pyramid):
        """åç¼©æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”ä¸ºå•å¼ å›¾åƒ - å‚è€ƒåº“å®ç°"""
        # ä»æœ€ç²—ç³™çš„å±‚å¼€å§‹
        img = image_pyramid[-1].copy()

        # é€å±‚ä¸Šé‡‡æ ·å¹¶ç´¯åŠ 
        for level in range(len(image_pyramid) - 2, -1, -1):
            target_shape = image_pyramid[level].shape
            size = (target_shape[1], target_shape[0])
            img = cv2.pyrUp(img, dstsize=size)
            img = img + image_pyramid[level]

        return img

    def collapse_laplacian_video_pyramid(self, vid_pyramid):
        """åç¼©æ‹‰æ™®æ‹‰æ–¯è§†é¢‘é‡‘å­—å¡” - å‚è€ƒåº“å®ç°"""
        print("åç¼©æ‹‰æ™®æ‹‰æ–¯è§†é¢‘é‡‘å­—å¡”...")
        frame_count = vid_pyramid[0].shape[0]
        result_frames = []

        for frame_idx in range(frame_count):
            # æå–å½“å‰å¸§çš„æ‰€æœ‰é‡‘å­—å¡”å±‚
            img_pyramid = [vid[frame_idx] for vid in vid_pyramid]

            # åç¼©é‡‘å­—å¡”
            collapsed = self.collapse_laplacian_pyramid(img_pyramid)
            result_frames.append(collapsed)

            if (frame_idx + 1) % 50 == 0:
                print(f"  å·²åç¼© {frame_idx + 1}/{frame_count} å¸§")

        return np.array(result_frames)

    def eulerian_magnification_correct(self, video_frames, fps, freq_low, freq_high,
                                      amplification, levels=4, skip_levels_at_top=2):
        """æ­£ç¡®çš„æ¬§æ‹‰è§†é¢‘æ”¾å¤§å®ç° - å®Œå…¨å‚è€ƒeulerian_magnificationåº“"""
        print(f"\n=== æ¬§æ‹‰è§†é¢‘æ”¾å¤§ï¼ˆæ­£ç¡®å®ç°ï¼‰ ===")
        print(f"å¸§æ•°: {len(video_frames)}, FPS: {fps}")
        print(f"é¢‘ç‡èŒƒå›´: {freq_low}-{freq_high} Hz")
        print(f"æ”¾å¤§å€æ•°: {amplification}x")
        print(f"é‡‘å­—å¡”å±‚æ•°: {levels}, è·³è¿‡é¡¶å±‚: {skip_levels_at_top}")

        # 1. æ„å»ºæ‹‰æ™®æ‹‰æ–¯è§†é¢‘é‡‘å­—å¡”
        vid_pyramid = self.create_laplacian_video_pyramid(video_frames, levels)

        # 2. å¯¹æ¯å±‚é‡‘å­—å¡”è¿›è¡Œæ—¶åŸŸå¸¦é€šæ»¤æ³¢å’Œæ”¾å¤§
        for level_idx in range(len(vid_pyramid)):
            # è·³è¿‡é¡¶å±‚ï¼ˆå™ªå£°å¤ªå¤šï¼‰å’Œåº•å±‚ï¼ˆé«˜æ–¯è¡¨ç¤ºï¼‰
            if level_idx < skip_levels_at_top or level_idx >= len(vid_pyramid) - 1:
                print(f"  è·³è¿‡ç¬¬ {level_idx} å±‚")
                continue

            print(f"  å¤„ç†ç¬¬ {level_idx} å±‚...")

            # åº”ç”¨FFTå¸¦é€šæ»¤æ³¢
            bandpassed = self.apply_temporal_bandpass_filter_fft(
                vid_pyramid[level_idx], fps, freq_low, freq_high, amplification
            )

            # å°†æ»¤æ³¢åçš„ä¿¡å·åŠ å›åŸå§‹é‡‘å­—å¡”å±‚
            vid_pyramid[level_idx] = vid_pyramid[level_idx] + bandpassed

        # 3. åç¼©é‡‘å­—å¡”é‡å»ºè§†é¢‘
        result_frames = self.collapse_laplacian_video_pyramid(vid_pyramid)

        # 4. è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
        result_frames = np.clip(result_frames, 0, 1)

        print("âœ… æ¬§æ‹‰è§†é¢‘æ”¾å¤§å®Œæˆ")
        return result_frames

    def _process_frame_batch_memory_safe(self, frame_batch, pyramid_buffers, mode, levels,
                                       freq_low, freq_high, amplification):
        """æµå¼å¤„ç†å·²åºŸå¼ƒ - ç›´æ¥è¿”å›åŸå§‹å¸§"""
        # æ³¨æ„ï¼šæµå¼å¤„ç†æ–¹æ³•å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨æ‰¹å¤„ç†æ–¹æ³•ï¼ˆload_video + magnify_motion/colorï¼‰
        # è¿™é‡Œåªæ˜¯ä¸ºäº†å…¼å®¹æ€§ï¼Œç›´æ¥è¿”å›åŸå§‹å¸§
        print("âš ï¸ è­¦å‘Šï¼šæµå¼å¤„ç†æ–¹æ³•å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨æ‰¹å¤„ç†æ–¹æ³•ä»¥è·å¾—æ­£ç¡®çš„è¿åŠ¨æ”¾å¤§æ•ˆæœ")
        return frame_batch

    def process_streaming(self, mode='motion', freq_low=0.4, freq_high=3.0,
                         amplification=10, levels=4, max_frames=None,
                         progress_callback=None):
        """å†…å­˜å®‰å…¨æµå¼å¤„ç†è§†é¢‘"""
        print(f"\nå¼€å§‹{mode}æ”¾å¤§å¤„ç†...")
        print(f"é¢‘ç‡èŒƒå›´: {freq_low}-{freq_high} Hz, æ”¾å¤§å€æ•°: {amplification}x")

        # å•å¸§å¤„ç†é¿å…å†…å­˜ç§¯ç´¯
        batch_size = 1

        # å†…å­˜ç›‘æ§
        import psutil
        import gc
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024

        # æ‰“å¼€è¾“å…¥è§†é¢‘
        cap = cv2.VideoCapture(self.video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {self.video_path}")

        # åˆ›å»ºä¸´æ—¶è¾“å‡ºæ–‡ä»¶ - 12Kåˆ†è¾¨ç‡éœ€è¦ç‰¹æ®Šå¤„ç†
        temp_video = self.output_path.replace('.mp4', '_temp.mp4')

        # è¶…é«˜åˆ†è¾¨ç‡ä½¿ç”¨æ›´ç¨³å®šçš„ç¼–ç å™¨
        if hasattr(self, 'extreme_mode') and self.extreme_mode:
            print("ğŸš¨ 12Kæ¨¡å¼ï¼šä½¿ç”¨æ— æŸç¼–ç å™¨é¿å…æ•°æ®æŸå")
            fourcc = cv2.VideoWriter_fourcc(*'FFV1')  # æ— æŸç¼–ç å™¨
        elif hasattr(self, 'is_ultra_high_res') and self.is_ultra_high_res:
            print("âš ï¸ 8K+æ¨¡å¼ï¼šä½¿ç”¨é«˜è´¨é‡ç¼–ç å™¨")
            fourcc = cv2.VideoWriter_fourcc(*'XVID')  # æ›´ç¨³å®šçš„ç¼–ç å™¨
        else:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')

        out = cv2.VideoWriter(temp_video, fourcc, self.fps, (self.width, self.height))

        # éªŒè¯VideoWriteræ˜¯å¦æˆåŠŸåˆå§‹åŒ–
        if not out.isOpened():
            print("âŒ VideoWriteråˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨ç¼–ç å™¨...")
            fourcc = cv2.VideoWriter_fourcc(*'MJPG')  # å¤‡ç”¨ç¼–ç å™¨
            out = cv2.VideoWriter(temp_video, fourcc, self.fps, (self.width, self.height))

            if not out.isOpened():
                raise ValueError(f"æ— æ³•åˆ›å»ºè¾“å‡ºè§†é¢‘æ–‡ä»¶: {temp_video}")

        # åˆå§‹åŒ–ç¼“å†²åŒº - ä¸ºæ¯ä¸ªé‡‘å­—å¡”å±‚çº§åˆ›å»ºç¼“å†²åŒº
        pyramid_buffers = [deque(maxlen=self.buffer_size) for _ in range(levels)]

        frame_count = 0
        max_process = max_frames if max_frames else self.total_frames

        # æ·»åŠ æ—¶é—´ç»Ÿè®¡
        import time
        start_time = time.time()
        last_update_time = start_time

        print(f"å¼€å§‹é«˜æ•ˆå¹¶è¡Œå¤„ç†ï¼Œæ‰¹å¤§å°: {batch_size}...")

        try:
            frame_batch = []

            while True:
                # è¯»å–å¸§
                ret, frame = cap.read()
                if not ret:
                    # å¤„ç†å‰©ä½™æ‰¹æ¬¡
                    if frame_batch:
                        print(f"\nå¤„ç†æœ€å {len(frame_batch)} å¸§...")
                        results = self._process_frame_batch_memory_safe(
                            frame_batch, pyramid_buffers, mode, levels,
                            freq_low, freq_high, amplification
                        )
                        # å†™å…¥ç»“æœ
                        for result in results:
                            output_uint8 = np.clip(result * 255, 0, 255).astype(np.uint8)
                            out.write(output_uint8)
                    print(f"\nè¯»å–å¸§å®Œæˆï¼Œå…±å¤„ç† {frame_count} å¸§")
                    break

                if frame_count >= max_process:
                    # å¤„ç†å‰©ä½™æ‰¹æ¬¡
                    if frame_batch:
                        print(f"\nå¤„ç†æœ€å {len(frame_batch)} å¸§...")
                        results = self._process_frame_batch_memory_safe(
                            frame_batch, pyramid_buffers, mode, levels,
                            freq_low, freq_high, amplification
                        )
                        # å†™å…¥ç»“æœ
                        for result in results:
                            output_uint8 = np.clip(result * 255, 0, 255).astype(np.uint8)
                            out.write(output_uint8)
                    print(f"\nè¾¾åˆ°æœ€å¤§å¸§æ•°é™åˆ¶: {max_process}")
                    break

                # è½¬æ¢å¸§æ ¼å¼
                frame_float = frame.astype(np.float32) / 255.0
                frame_batch.append(frame_float)

                # æ›´æ–°ç¼“å†²åŒºï¼ˆå•çº¿ç¨‹æ›´æ–°ï¼Œé¿å…ç«äº‰æ¡ä»¶ï¼‰
                self._update_buffers(frame_float, pyramid_buffers, mode, levels)

                frame_count += 1
                # è¶…é«˜åˆ†è¾¨ç‡æ¨¡å¼ä¸‹æ›´é¢‘ç¹çš„åƒåœ¾å›æ”¶
                if hasattr(self, 'extreme_mode') and self.extreme_mode:
                    if frame_count % 1 == 0:  # æ¯å¸§éƒ½æ¸…ç†
                        gc.collect()
                elif hasattr(self, 'is_ultra_high_res') and self.is_ultra_high_res:
                    if frame_count % 2 == 0:  # æ¯2å¸§æ¸…ç†
                        gc.collect()
                else:
                    if frame_count % 5 == 0:
                        gc.collect()

                # å½“æ‰¹æ¬¡æ»¡äº†æ—¶å¤„ç†
                if len(frame_batch) >= batch_size:
                    # å†…å­˜å®‰å…¨æ‰¹å¤„ç†
                    results = self._process_frame_batch_memory_safe(
                        frame_batch, pyramid_buffers, mode, levels,
                        freq_low, freq_high, amplification
                    )

                    # å†™å…¥ç»“æœ
                    for result in results:
                        output_uint8 = np.clip(result * 255, 0, 255).astype(np.uint8)
                        out.write(output_uint8)

                    frame_batch = []
                    current_time = time.time()

                    # æ›´é¢‘ç¹çš„è¿›åº¦æ›´æ–°
                    if progress_callback and (current_time - last_update_time >= 2.0):
                        try:
                            progress = (frame_count / max_process) * 100
                            elapsed = current_time - start_time
                            fps = frame_count / elapsed if elapsed > 0 else 0
                            eta = (max_process - frame_count) / fps if fps > 0 else 0

                            progress_msg = f"å¹¶è¡Œå¤„ç†: {frame_count}/{max_process} ({progress:.1f}%) - {fps:.1f} FPS - ETA: {eta:.0f}s"
                            progress_callback(progress_msg)
                            last_update_time = current_time
                            print(f"\nè¿›åº¦æ›´æ–°: {progress_msg}")
                        except Exception as e:
                            print(f"\nè¿›åº¦æ›´æ–°å‡ºé”™: {e}")

                    # è¿›åº¦æ˜¾ç¤ºå’Œå†…å­˜ç®¡ç†
                    if frame_count % 50 == 0:
                        current_memory = process.memory_info().rss / 1024 / 1024
                        memory_growth = current_memory - initial_memory
                        elapsed = current_time - start_time
                        fps = frame_count / elapsed if elapsed > 0 else 0

                        print(f"å·²å¤„ç†: {frame_count}/{max_process} å¸§ - {fps:.1f} FPS")

                        # å†…å­˜è¶…è¿‡é˜ˆå€¼æ—¶æ¸…ç†
                        if memory_growth > 1000:
                            gc.collect()
                            for buf in pyramid_buffers:
                                buf.clear()

        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­å¤„ç†")
        finally:
            cap.release()
            out.release()
            # æ¸…ç†çº¿ç¨‹æ± 
            self._cleanup_executor()

        # æœ€ç»ˆç»Ÿè®¡
        total_time = time.time() - start_time
        avg_fps = frame_count / total_time if total_time > 0 else 0

        print(f"é«˜æ•ˆå¹¶è¡Œå¤„ç†å®Œæˆ: {frame_count} å¸§")
        print(f"æ€»è€—æ—¶: {total_time:.1f}ç§’, å¹³å‡é€Ÿåº¦: {avg_fps:.1f} FPS")
        print(f"çº¿ç¨‹æ•°: {self.num_workers}, ç†è®ºåŠ é€Ÿæ¯”: {self.num_workers}x")
        print(f"å®é™…åŠ é€Ÿæ¯”: {avg_fps / max(1, avg_fps / self.num_workers):.1f}x")
        return temp_video

    def _update_buffers(self, frame_float, pyramid_buffers, mode, levels):
        """æ›´æ–°ç¼“å†²åŒºï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰- ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”"""
        if mode == 'color':
            frame_ycrcb = cv2.cvtColor(frame_float, cv2.COLOR_BGR2YCrCb)
            pyramid_buffers[0].append(frame_ycrcb)
        else:
            # ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”è€Œä¸æ˜¯é«˜æ–¯é‡‘å­—å¡”
            pyramid = self.build_laplacian_pyramid(frame_float, levels)
            for level in range(levels):
                pyramid_buffers[level].append(pyramid[level])

    def magnify_motion_streaming(self, fps, freq_low=0.4, freq_high=3.0,
                               amplification=10, levels=4, max_frames=None,
                               progress_callback=None):
        """é«˜æ•ˆå¹¶è¡Œæµå¼è¿åŠ¨æ”¾å¤§"""
        temp_video = self.process_streaming(
            mode='motion', freq_low=freq_low, freq_high=freq_high,
            amplification=amplification, levels=levels, max_frames=max_frames,
            progress_callback=progress_callback
        )
        return temp_video

    def magnify_color_streaming(self, fps, freq_low=0.4, freq_high=3.0,
                              amplification=20, max_frames=None,
                              progress_callback=None):
        """é«˜æ•ˆå¹¶è¡Œæµå¼è‰²å½©æ”¾å¤§"""
        temp_video = self.process_streaming(
            mode='color', freq_low=freq_low, freq_high=freq_high,
            amplification=amplification, levels=4, max_frames=max_frames,
            progress_callback=progress_callback
        )
        return temp_video

    def generate_output_filename(self, mode, freq_low, freq_high, amplification, output_format='mp4'):
        """ç”Ÿæˆå¸¦æ—¶é—´æˆ³å’Œå‚æ•°çš„è¾“å‡ºæ–‡ä»¶å"""
        import os
        from datetime import datetime

        # è·å–åŸå§‹æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        base_name = os.path.splitext(os.path.basename(self.video_path))[0]

        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ç”Ÿæˆå‚æ•°å­—ç¬¦ä¸²
        params = f"{mode}_amp{amplification}_freq{freq_low}-{freq_high}"

        # æ ¹æ®æ ¼å¼è®¾ç½®æ‰©å±•å
        format_extensions = {
            'mp4': 'mp4',
            'prores_proxy': 'mov',
            'prores_lt': 'mov',
            'prores_standard': 'mov',
            'prores_hq': 'mov',
            'prores_4444': 'mov',
            'prores_4444xq': 'mov'
        }

        ext = format_extensions.get(output_format, 'mp4')
        filename = f"{base_name}_{timestamp}_{params}.{ext}"

        # æ›´æ–°è¾“å‡ºè·¯å¾„
        output_dir = os.path.dirname(self.output_path)
        self.output_path = os.path.join(output_dir, filename)

        return self.output_path

    def _validate_temp_video(self, temp_video_path):
        """éªŒè¯ä¸´æ—¶è§†é¢‘æ–‡ä»¶å®Œæ•´æ€§"""
        try:
            if not os.path.exists(temp_video_path):
                return False

            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(temp_video_path)
            if file_size == 0:
                return False

            # å°è¯•ç”¨OpenCVè¯»å–ç¬¬ä¸€å¸§éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
            cap = cv2.VideoCapture(temp_video_path)
            if not cap.isOpened():
                return False

            ret, frame = cap.read()
            cap.release()

            return ret and frame is not None

        except Exception as e:
            print(f"éªŒè¯ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return False

    def save_video(self, temp_video_path, audio_source=None, output_format='mp4', mode='motion',
                   freq_low=0.4, freq_high=3.0, amplification=10):
        """ä¿å­˜æœ€ç»ˆè§†é¢‘ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰"""

        # ç”Ÿæˆè‡ªå®šä¹‰æ–‡ä»¶å
        final_path = self.generate_output_filename(mode, freq_low, freq_high, amplification, output_format)
        print(f"\nä¿å­˜è§†é¢‘åˆ°: {final_path}")

        # ProResç¼–ç å™¨é…ç½®
        prores_configs = {
            'prores_proxy': ['-c:v', 'prores_ks', '-profile:v', '0'],  # ProRes Proxy
            'prores_lt': ['-c:v', 'prores_ks', '-profile:v', '1'],     # ProRes LT
            'prores_standard': ['-c:v', 'prores_ks', '-profile:v', '2'], # ProRes Standard
            'prores_hq': ['-c:v', 'prores_ks', '-profile:v', '3'],     # ProRes HQ
            'prores_4444': ['-c:v', 'prores_ks', '-profile:v', '4'],   # ProRes 4444
            'prores_4444xq': ['-c:v', 'prores_ks', '-profile:v', '5']  # ProRes 4444 XQ
        }

        try:
            # æ£€æµ‹è¶…é«˜åˆ†è¾¨ç‡å¹¶æ·»åŠ FFmpegä¼˜åŒ–å‚æ•°
            is_ultra_high_res = hasattr(self, 'is_ultra_high_res') and self.is_ultra_high_res
            is_extreme_mode = hasattr(self, 'extreme_mode') and self.extreme_mode

            # è¶…é«˜åˆ†è¾¨ç‡FFmpegä¼˜åŒ–å‚æ•°
            ultra_high_res_params = []
            if is_extreme_mode:
                print("ğŸš¨ 12Kæ¨¡å¼ï¼šä½¿ç”¨FFmpegæé™ä¼˜åŒ–å‚æ•°")
                ultra_high_res_params = [
                    '-threads', '0',  # ä½¿ç”¨æ‰€æœ‰CPUçº¿ç¨‹
                    '-thread_type', 'frame+slice',  # å¸§çº§å’Œç‰‡çº§å¹¶è¡Œ
                    '-max_muxing_queue_size', '9999',  # å¢å¤§ç¼“å†²åŒº
                    '-bufsize', '20M',  # å¢å¤§ç¼–ç ç¼“å†²åŒº
                    '-maxrate', '200M'  # å¢å¤§æœ€å¤§ç ç‡
                ]
            elif is_ultra_high_res:
                print("âš ï¸ 8K+æ¨¡å¼ï¼šä½¿ç”¨FFmpegé«˜åˆ†è¾¨ç‡ä¼˜åŒ–å‚æ•°")
                ultra_high_res_params = [
                    '-threads', '0',
                    '-max_muxing_queue_size', '4096',
                    '-bufsize', '10M'
                ]

            if output_format in prores_configs:
                # ProResæ ¼å¼ - è¶…é«˜åˆ†è¾¨ç‡ä¼˜åŒ–
                video_codec = prores_configs[output_format]
                if audio_source:
                    cmd = [
                        'ffmpeg', '-y', '-i', temp_video_path, '-i', audio_source,
                        *ultra_high_res_params,
                        *video_codec, '-c:a', 'pcm_s16le', '-shortest',
                        final_path
                    ]
                else:
                    cmd = [
                        'ffmpeg', '-y', '-i', temp_video_path,
                        *ultra_high_res_params,
                        *video_codec,
                        final_path
                    ]
            else:
                # MP4æ ¼å¼ - è¶…é«˜åˆ†è¾¨ç‡ä¼˜åŒ–
                h264_params = ['-preset', 'medium', '-crf', '23']
                if is_extreme_mode:
                    # 12Kæ¨¡å¼ä½¿ç”¨æ›´å¿«çš„é¢„è®¾å’Œæ›´é«˜çš„CRF
                    h264_params = ['-preset', 'ultrafast', '-crf', '28']
                elif is_ultra_high_res:
                    # 8Kæ¨¡å¼ä½¿ç”¨å¿«é€Ÿé¢„è®¾
                    h264_params = ['-preset', 'fast', '-crf', '25']

                if audio_source:
                    cmd = [
                        'ffmpeg', '-y', '-i', temp_video_path, '-i', audio_source,
                        *ultra_high_res_params,
                        '-c:v', 'libx264', *h264_params, '-c:a', 'aac', '-shortest',
                        final_path
                    ]
                else:
                    cmd = [
                        'ffmpeg', '-y', '-i', temp_video_path,
                        *ultra_high_res_params,
                        '-c:v', 'libx264', *h264_params,
                        final_path
                    ]

            print(f"æ‰§è¡ŒFFmpegå‘½ä»¤: {' '.join(cmd[:8])}...")  # åªæ˜¾ç¤ºå‰8ä¸ªå‚æ•°é¿å…è¿‡é•¿
            result = subprocess.run(cmd, capture_output=True, text=True)

            if result.returncode != 0:
                print(f"FFmpegé”™è¯¯: {result.stderr}")

                # æ£€æŸ¥æ˜¯å¦æ˜¯è¶…é«˜åˆ†è¾¨ç‡å¯¼è‡´çš„é—®é¢˜
                if "Invalid data found when processing input" in result.stderr:
                    print("ğŸš¨ æ£€æµ‹åˆ°è¶…é«˜åˆ†è¾¨ç‡æ•°æ®æŸåé—®é¢˜ï¼Œå°è¯•ä¿®å¤...")

                    # éªŒè¯ä¸´æ—¶æ–‡ä»¶å®Œæ•´æ€§
                    if self._validate_temp_video(temp_video_path):
                        print("âœ… ä¸´æ—¶æ–‡ä»¶å®Œæ•´ï¼Œä½¿ç”¨å¤‡ç”¨è½¬æ¢æ–¹æ³•...")
                        # ä½¿ç”¨æ›´ä¿å®ˆçš„FFmpegå‚æ•°é‡è¯•
                        fallback_cmd = [
                            'ffmpeg', '-y', '-err_detect', 'ignore_err',
                            '-i', temp_video_path,
                            '-c:v', 'libx264', '-preset', 'ultrafast', '-crf', '30',
                            '-avoid_negative_ts', 'make_zero',
                            final_path
                        ]
                        fallback_result = subprocess.run(fallback_cmd, capture_output=True, text=True)

                        if fallback_result.returncode == 0:
                            print("âœ… å¤‡ç”¨è½¬æ¢æˆåŠŸ")
                        else:
                            print("âŒ å¤‡ç”¨è½¬æ¢ä¹Ÿå¤±è´¥ï¼Œç›´æ¥å¤åˆ¶ä¸´æ—¶æ–‡ä»¶")
                            import shutil
                            shutil.copy2(temp_video_path, final_path)
                    else:
                        print("âŒ ä¸´æ—¶æ–‡ä»¶æŸåï¼Œå¤„ç†å¤±è´¥")
                        raise ValueError("ä¸´æ—¶è§†é¢‘æ–‡ä»¶æŸåï¼Œå¯èƒ½æ˜¯å†…å­˜ä¸è¶³æˆ–åˆ†è¾¨ç‡è¿‡é«˜")
                else:
                    # å…¶ä»–FFmpegé”™è¯¯ï¼Œç›´æ¥å¤åˆ¶ä¸´æ—¶æ–‡ä»¶
                    import shutil
                    shutil.copy2(temp_video_path, final_path)
                    print("ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ä½œä¸ºè¾“å‡º")
            else:
                print("âœ… FFmpegè½¬æ¢æˆåŠŸ")

            # éªŒè¯æœ€ç»ˆè¾“å‡ºæ–‡ä»¶
            if os.path.exists(final_path) and os.path.getsize(final_path) > 0:
                print(f"âœ… è¾“å‡ºæ–‡ä»¶éªŒè¯é€šè¿‡: {os.path.getsize(final_path)} å­—èŠ‚")
            else:
                print("âŒ è¾“å‡ºæ–‡ä»¶éªŒè¯å¤±è´¥")

            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_video_path):
                os.remove(temp_video_path)

        except Exception as e:
            print(f"ä¿å­˜è§†é¢‘æ—¶å‡ºé”™: {e}")
            # å¦‚æœå‡ºé”™ï¼Œå°è¯•ç›´æ¥å¤åˆ¶ä¸´æ—¶æ–‡ä»¶
            try:
                import shutil
                shutil.copy2(temp_video_path, self.output_path)
                print("ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ä½œä¸ºè¾“å‡º")
            except:
                print("æ— æ³•ä¿å­˜è§†é¢‘æ–‡ä»¶")
                raise

        print(f"å®Œæˆ! è§†é¢‘å·²ä¿å­˜: {self.output_path}")

    # ä¿ç•™æ—§çš„æ¥å£ä»¥å…¼å®¹ç°æœ‰ä»£ç 
    def load_video(self, max_frames=None):
        """åŠ è½½è§†é¢‘å¸§åˆ°å†…å­˜"""
        print(f"åŠ è½½è§†é¢‘: {self.video_path}")
        if max_frames and max_frames > 500:
            print(f"âš ï¸ å¸§æ•°è¿‡å¤š({max_frames})ï¼Œå¼ºåˆ¶é™åˆ¶ä¸º500å¸§ä»¥é¿å…å†…å­˜æº¢å‡º")
            max_frames = 500

        cap = cv2.VideoCapture(self.video_path)
        frames = []
        frame_count = 0
        max_load = max_frames if max_frames else min(500, self.total_frames)

        while True:
            ret, frame = cap.read()
            if not ret or frame_count >= max_load:
                break
            frame = frame.astype(np.float32) / 255.0
            frames.append(frame)
            frame_count += 1

            if frame_count % 50 == 0:
                print(f"  å·²åŠ è½½ {frame_count}/{max_load} å¸§")

        cap.release()
        print(f"âœ… åŠ è½½å®Œæˆ: {len(frames)} å¸§")
        return np.array(frames)

    def magnify_motion(self, frames, fps, freq_low=0.4, freq_high=3.0,
                       amplification=10, levels=4, skip_levels_at_top=2):
        """è¿åŠ¨æ”¾å¤§ - ä½¿ç”¨æ­£ç¡®çš„æ¬§æ‹‰è§†é¢‘æ”¾å¤§ç®—æ³•"""
        print(f"\n=== è¿åŠ¨æ”¾å¤§ ===")
        return self.eulerian_magnification_correct(
            frames, fps, freq_low, freq_high, amplification, levels, skip_levels_at_top
        )

    def magnify_color(self, frames, fps, freq_low=0.4, freq_high=3.0,
                     amplification=20, levels=4, skip_levels_at_top=2):
        """è‰²å½©æ”¾å¤§ - ä½¿ç”¨æ­£ç¡®çš„æ¬§æ‹‰è§†é¢‘æ”¾å¤§ç®—æ³•"""
        print(f"\n=== è‰²å½©æ”¾å¤§ ===")
        # è‰²å½©æ”¾å¤§é€šå¸¸ä½¿ç”¨æ›´é«˜çš„æ”¾å¤§å€æ•°å’Œæ›´å®½çš„é¢‘ç‡èŒƒå›´
        return self.eulerian_magnification_correct(
            frames, fps, freq_low, freq_high, amplification, levels, skip_levels_at_top
        )

    def save_video_from_frames(self, frames, audio_source=None, output_format='mp4', mode='motion',
                              freq_low=0.4, freq_high=3.0, amplification=10):
        """ä»å¸§æ•°ç»„ä¿å­˜è§†é¢‘"""
        print(f"\nä¿å­˜è§†é¢‘...")

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        final_path = self.generate_output_filename(mode, freq_low, freq_high, amplification, output_format)

        # åˆ›å»ºä¸´æ—¶è§†é¢‘æ–‡ä»¶
        temp_video = final_path.replace('.mp4', '_temp.mp4').replace('.mov', '_temp.mov')
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(temp_video, fourcc, self.fps, (self.width, self.height))

        if not out.isOpened():
            raise ValueError(f"æ— æ³•åˆ›å»ºä¸´æ—¶è§†é¢‘æ–‡ä»¶: {temp_video}")

        # å†™å…¥å¸§
        for idx, frame in enumerate(frames):
            output_uint8 = np.clip(frame * 255, 0, 255).astype(np.uint8)
            out.write(output_uint8)

            if (idx + 1) % 50 == 0:
                print(f"  å·²å†™å…¥ {idx + 1}/{len(frames)} å¸§")

        out.release()
        print(f"âœ… ä¸´æ—¶è§†é¢‘å·²åˆ›å»º: {temp_video}")

        # ä½¿ç”¨FFmpegè½¬æ¢ä¸ºæœ€ç»ˆæ ¼å¼
        self.save_video(temp_video, audio_source, output_format, mode, freq_low, freq_high, amplification)

        return final_path